{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "il = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").load(table=\"inductive_loops\", keyspace=\"dice\")\n",
    "il1 = il"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "il = il1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "hourTokens = udf(lambda time: float(time.hour), DoubleType())\n",
    "il = il.withColumn('hour', hourTokens(il.updated))\n",
    "minuteTokens = udf(lambda time: float(time.minute), DoubleType())\n",
    "il = il.withColumn('minute', minuteTokens(il.updated))\n",
    "weekTokens = udf(lambda time: float(time.weekday()), DoubleType())\n",
    "il = il.withColumn('weekday', weekTokens(il.updated))\n",
    "\n",
    "il = il.withColumn('stat_double', il['stat'].cast(\"double\"))\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"id\", outputCol=\"id_double\").fit(il)\n",
    "il = indexer.transform(il)\n",
    "\n",
    "il = il.fillna({'lanedescription': ' '})\n",
    "indexer = StringIndexer(inputCol=\"lanedescription\", outputCol=\"lane\").fit(il)\n",
    "il = indexer.transform(il)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "il = il.select('id_double','avgspeed', 'hour', 'minute', 'weekday', 'lane', 'stat_double').where('stat != 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "not_use = ['avgspeed', 'stat_double']\n",
    "assembler = VectorAssembler(inputCols=[x for x in il.columns if x not in not_use], outputCol='features')\n",
    "il = assembler.transform(il)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id_double=13.0, avgspeed=0, hour=0.0, minute=25.0, weekday=1.0, lane=0.0, stat_double=6.0, features=DenseVector([13.0, 0.0, 25.0, 1.0, 0.0]))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = il.randomSplit([0.7, 0.3], 1234)\n",
    "train.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(maxDepth=30, maxBins=50, minInstancesPerNode=25, labelCol='avgspeed')\n",
    "model = dt.fit(train) #rmse=0.495445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 10.9611\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(labelCol=\"avgspeed\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", labelCol='stat_double')\n",
    "model = nb.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(maxDepth=30, maxBins=50, minInstancesPerNode=10, labelCol='stat_double')\n",
    "model = dt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.831538305223\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "predictions = model.transform(test)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"stat_double\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}